{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374b7c01-40eb-4294-bdff-bd3f81e03aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: duckdb in /home/nikita/.local/lib/python3.10/site-packages (0.9.2)\n",
      "Collecting polars\n",
      "  Obtaining dependency information for polars from https://files.pythonhosted.org/packages/39/e7/1879249e826b17a2f1c5c2629c0d2b58a170606dace66f072d52a2ad1a7b/polars-0.19.17-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading polars-0.19.17-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting pydantic\n",
      "  Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/0a/2b/64066de1c4cf3d4ed623beeb3bbf3f8d0cc26661f1e7d180ec5eb66b75a5/pydantic-2.5.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m994.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic)\n",
      "  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/7c/f5/3e59681bd53955da311a7f4efbb6315d01006e9d18b8a06b527a22d3d923/pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/nikita/.local/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Downloading polars-0.19.17-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m903.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: pydantic-core, polars, annotated-types, pydantic\n",
      "Successfully installed annotated-types-0.6.0 polars-0.19.17 pydantic-2.5.2 pydantic-core-2.14.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb polars pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46edc45f-1276-4eab-8450-c8b2fab581e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from itertools import zip_longest\n",
    "from random import randint, choice, uniform\n",
    "from typing import List, Dict, NamedTuple, Tuple\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import psutil\n",
    "import time\n",
    "import sys\n",
    "import tracemalloc\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c9ffd6-c926-4de8-9909-9499a426cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseType(NamedTuple):\n",
    "    polars = 'polars'\n",
    "    duckdb = 'duckdb'\n",
    "    sqlite3 = 'sqlite3'\n",
    "    dictdb = 'dictdb'\n",
    "\n",
    "\n",
    "class TotalTestResult(BaseModel):\n",
    "    storage_name: str\n",
    "    avg_time: float\n",
    "    avg_memory: float\n",
    "\n",
    "\n",
    "class AbstractWarehouse(ABC):\n",
    "    @abstractmethod\n",
    "    def load_schema(self, *args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_data(self, *args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_row(self, *args):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Polars(AbstractWarehouse):\n",
    "    def __init__(self):\n",
    "        self.DB = {}\n",
    "\n",
    "    def load_schema(self, schema: Dict[str, Tuple[dict, list]]):\n",
    "        for table in schema:\n",
    "            cols = schema[table][0]\n",
    "            self.DB[table] = pl.DataFrame(schema=cols)\n",
    "\n",
    "    def read_data(self, table_name: str, sort: Dict, filter: Dict, fetchall=False):\n",
    "        filter_expr = None\n",
    "        for k, v in filter.items():\n",
    "            cond = pl.col(k) == v\n",
    "            if filter_expr is None:\n",
    "                filter_expr = cond\n",
    "            else:\n",
    "                filter_expr &= cond\n",
    "\n",
    "        if filter:\n",
    "            filter_result = self.DB[table_name].filter(filter_expr)\n",
    "        else:\n",
    "            filter_result = self.DB\n",
    "\n",
    "        sort_cols = list(sort.keys())\n",
    "        order = [elem == 'desc' for elem in sort.values()]\n",
    "        if len(order) == 1:\n",
    "            order = order[0]\n",
    "\n",
    "        if sort:\n",
    "            sort_result = filter_result.sort(*sort_cols, descending=order)\n",
    "        else:\n",
    "            sort_result = filter_result\n",
    "\n",
    "        if fetchall:\n",
    "            return sort_result.to_dicts()\n",
    "\n",
    "        if not sort_result.is_empty():\n",
    "            return sort_result[0].to_dicts()[0]\n",
    "\n",
    "    def add_row(self, data: dict, table_name: str):\n",
    "        cols, indx = tables_schema[table_name]\n",
    "        schema = self.DB[table_name].schema\n",
    "        self.DB[table_name] = pl.concat([self.DB[table_name], pl.from_dict(data, schema=schema)], rechunk=True,\n",
    "                                        how=\"diagonal\")\n",
    "\n",
    "\n",
    "class SQLite3(AbstractWarehouse):\n",
    "    types = {\n",
    "        str: 'text',\n",
    "        'numeric': 'numeric',\n",
    "        int: 'int',\n",
    "        float: 'numeric'\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect(':memory:')\n",
    "\n",
    "    def load_schema(self, schema: Dict[str, Tuple[dict, list]]):\n",
    "        for table in schema:\n",
    "            table_schema = schema[table]\n",
    "            cols = table_schema[0]\n",
    "            indexes = set(i[0] for i in table_schema[1] if i[0])\n",
    "\n",
    "            cols_expr = ', '.join([f'\\'{c}\\' {self.types[cols[c]]}' for c in cols])\n",
    "            self.conn.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            self.conn.execute(f\"CREATE TABLE {table} ({cols_expr})\")\n",
    "\n",
    "            for idx in indexes:\n",
    "                idx_name = \"_\".join(idx)\n",
    "                idx_list = \", \".join(idx)\n",
    "                self.conn.execute(f\"create index {table}_{idx_name}_idx on {table} ({idx_list})\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def read_data(self, table_name, sort: Dict, filter: Dict, fetchall: bool = False):\n",
    "        filter_conds = [\n",
    "            f'({k}=${k} or {k} is null)' if isinstance(v, list) else (f'{k} is null' if v is None else f'{k}=${k}')\n",
    "            for\n",
    "            k, v in filter.items()]\n",
    "        query = f\"SELECT * FROM {table_name} {'WHERE' if filter_conds else ''} \" + ' AND '.join(filter_conds)\n",
    "        sort_conds = [f'{field_sort} {order_sort}' for field_sort, order_sort in sort.items()]\n",
    "        args = filter\n",
    "        if sort_conds:\n",
    "            query += f\"\"\" ORDER BY {','.join(sort_conds)}\"\"\"\n",
    "        if fetchall:\n",
    "            result = self.conn.execute(query, args).fetchall()\n",
    "            if result:\n",
    "                result = [dict(zip(tables_schema[table_name][0].keys(), r)) for r in result]\n",
    "        else:\n",
    "            result = self.conn.execute(query, args).fetchone()\n",
    "            if result:\n",
    "                result = dict(zip(tables_schema[table_name][0].keys(), result))\n",
    "        return result\n",
    "\n",
    "    def add_row(self, data: dict, table_name: str):\n",
    "        val_expr = ','.join(['?'] * len(data.keys()))\n",
    "        cols = sorted(data.keys())\n",
    "        cols_expr = ','.join([f\"\\'{col}\\'\" for col in cols])\n",
    "\n",
    "        self.conn.execute(f\"INSERT INTO {table_name} ({cols_expr}) VALUES ({val_expr})\",\n",
    "                          tuple([data[c] for c in cols]))\n",
    "        self.conn.commit()\n",
    "\n",
    "\n",
    "class DuckDB(AbstractWarehouse):\n",
    "    def __init__(self):\n",
    "        self.conn = duckdb.connect()\n",
    "\n",
    "    types = {\n",
    "        str: 'text',\n",
    "        'numeric': 'numeric',\n",
    "        int: 'int',\n",
    "        float: 'numeric'\n",
    "    }\n",
    "\n",
    "    def load_schema(self, schema: Dict[str, Tuple[dict, list]]):\n",
    "        for table in schema:\n",
    "            table_schema = schema[table]\n",
    "            cols = table_schema[0]\n",
    "            indexes = set(i[0] for i in table_schema[1] if i[0])\n",
    "\n",
    "            cols_expr = ', '.join([f'{c} {self.types[cols[c]]}' for c in cols])\n",
    "            self.conn.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            self.conn.execute(f\"CREATE TABLE {table} ({cols_expr})\")\n",
    "\n",
    "            for idx in indexes:\n",
    "                idx_name = \"_\".join(idx)\n",
    "                idx_list = \", \".join(idx)\n",
    "                self.conn.execute(f\"create index {table}_{idx_name}_idx on {table} ({idx_list})\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def read_data(self, table_name, sort: Dict, filter: Dict, fetchall: bool = False):\n",
    "        filter_conds = [\n",
    "            f'({k}=${k} or {k} is null)' if isinstance(v, list) else (f'{k} is null' if v is None else f'{k}=${k}')\n",
    "            for\n",
    "            k, v in filter.items()]\n",
    "        query = f\"\"\"SELECT * FROM {table_name} {'WHERE' if filter_conds else ''} \"\"\" + ' AND '.join(filter_conds)\n",
    "        sort_conds = [f'{field_sort} {order_sort}' for field_sort, order_sort in sort.items()]\n",
    "        if sort_conds:\n",
    "            query += f\"\"\" ORDER BY {','.join(sort_conds)}\"\"\"\n",
    "        args = filter\n",
    "        if fetchall:\n",
    "            result = self.conn.execute(query, args).fetchall()\n",
    "            if result:\n",
    "                result = [dict(zip(tables_schema[table_name][0].keys(), r)) for r in result]\n",
    "        else:\n",
    "            result = self.conn.execute(query, args).fetchone()\n",
    "            if result:\n",
    "                result = dict(zip(tables_schema[table_name][0].keys(), result))\n",
    "        return result\n",
    "\n",
    "    def add_row(self, data: dict, table_name: str):\n",
    "        val_expr = ','.join(['?'] * len(data.keys()))\n",
    "        cols = sorted(data.keys())\n",
    "        cols_expr = ','.join([f\"{col}\" for col in cols])\n",
    "\n",
    "        self.conn.execute(f\"INSERT INTO {table_name} ({cols_expr}) VALUES ({val_expr})\",\n",
    "                          tuple([data[c] for c in cols]))\n",
    "        self.conn.commit()\n",
    "\n",
    "\n",
    "class DictDB(AbstractWarehouse):\n",
    "    def __init__(self):\n",
    "        self.DICT_DB = {}\n",
    "        self.schema = {}\n",
    "\n",
    "    def load_schema(self, schema: Dict[str, Tuple[dict, list]]):\n",
    "        self.schema = schema\n",
    "        for table in schema:\n",
    "            table_schema = schema[table]\n",
    "            indexes = table_schema[1]\n",
    "\n",
    "            self.DICT_DB[table] = dict()\n",
    "            for idx in indexes:\n",
    "                self.DICT_DB[table][self._sort_index(idx)] = dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_filter(search_filter: Dict) -> Tuple:\n",
    "        return tuple(sorted(search_filter.keys()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_index(index: Tuple[Tuple]) -> Tuple:\n",
    "        return tuple(sorted(index[0])), index[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def _multi_key_sort(dict_list: List, sort_fields: Tuple):\n",
    "        for key, order in reversed(sort_fields):\n",
    "            dict_list.sort(key=itemgetter(key), reverse=order == 'desc')\n",
    "        return dict_list\n",
    "\n",
    "    def filter_sort_data(self, table_name: str, filter: Dict, sort: Tuple):\n",
    "        filter_fields = self._sort_filter(filter)\n",
    "        if not sort:\n",
    "            return\n",
    "        filter_result = self.DICT_DB[table_name].get((filter_fields, sort), {}).get(\n",
    "            tuple(filter.values()), [])\n",
    "        if filter_result is not None:\n",
    "            sorted_result = self._multi_key_sort(filter_result, sort)\n",
    "            self.DICT_DB[table_name].get((filter_fields, sort), {})[\n",
    "                tuple(filter.values())] = sorted_result\n",
    "\n",
    "    def read_data(self, table_name, sort: Dict = {}, filter: Dict = {}, fetchall: bool = False):\n",
    "        if not self.DICT_DB.get(table_name):\n",
    "            raise ValueError('No such table')\n",
    "        sorted_filter_fields = self._sort_filter(filter)\n",
    "        filter_index = (sorted_filter_fields,\n",
    "                        tuple([(sort_keys, sort_values) for sort_keys, sort_values in sort.items()]))\n",
    "        if not self.DICT_DB[table_name].get(filter_index):\n",
    "            raise ValueError('No such index')\n",
    "\n",
    "        result = self.DICT_DB[table_name][filter_index].get(tuple([filter[k] for k in sorted_filter_fields]), [])\n",
    "\n",
    "        if fetchall or not result:\n",
    "            return result\n",
    "        else:\n",
    "            return result[0]\n",
    "\n",
    "    def add_row(self, data: Dict, table_name: str):\n",
    "        cols, indx = self.schema[table_name]\n",
    "\n",
    "        for idx in indx:\n",
    "            sort_idx = self._sort_index(idx)\n",
    "            self.DICT_DB[table_name][sort_idx].setdefault(tuple([data[k] for k in sort_idx[0]]), []).append({k: data[k] for k in data if k not in sort_idx[0]})\n",
    "\n",
    "            filter = {k: data[k] for k in sort_idx[0]}\n",
    "            self.filter_sort_data(table_name=table_name, filter=filter, sort=idx[1])\n",
    "\n",
    "\n",
    "class WarehouseManager(AbstractWarehouse):\n",
    "    def __init__(self, warehouse_type: str, schema: dict):\n",
    "        self.schema = schema\n",
    "        self.warehouse_type = warehouse_type\n",
    "        self.warehouse = self._get_warehouse()\n",
    "\n",
    "    def _get_warehouse(self):\n",
    "        if self.warehouse_type == WarehouseType.sqlite3:\n",
    "            return SQLite3()\n",
    "        elif self.warehouse_type == WarehouseType.polars:\n",
    "            return Polars()\n",
    "        elif self.warehouse_type == WarehouseType.duckdb:\n",
    "            return DuckDB()\n",
    "        elif self.warehouse_type == WarehouseType.dictdb:\n",
    "            return DictDB()\n",
    "        else:\n",
    "            raise ValueError(\"Warehouse not found!\")\n",
    "\n",
    "    def load_schema(self):\n",
    "        self.warehouse.load_schema(schema=self.schema)\n",
    "\n",
    "    def read_data(self, table_name: str, filter: Dict, sort: Dict = {}, fetchall: bool = False):\n",
    "        data = self.warehouse.read_data(table_name, sort, filter, fetchall)\n",
    "        return data\n",
    "\n",
    "    def add_row(self, table_name: str, data: dict):\n",
    "        self.warehouse.add_row(data, table_name)\n",
    "\n",
    "    def add_selective_data(self, table_name: str, records_count: int):\n",
    "        data = self._get_random_data(table_name)\n",
    "        for i in range(records_count):\n",
    "            self.warehouse.add_row(data, table_name)\n",
    "\n",
    "    def add_random_data(self, table_name: str, records_count: int):\n",
    "        for i in range(records_count):\n",
    "            data = self._get_random_data(table_name)\n",
    "            self.warehouse.add_row(data, table_name)\n",
    "\n",
    "    symbol_list = [chr(i) for i in range(0x30, 0x39)]\n",
    "    code_length = 10\n",
    "\n",
    "    def _get_random_value_by_type(self, data_type):\n",
    "        if data_type == str:\n",
    "            return ''.join([choice(self.symbol_list) for i in range(self.code_length)])\n",
    "        elif data_type == int:\n",
    "            return randint(0, 255)\n",
    "        elif data_type == float:\n",
    "            return round(uniform(0, 100), 2)\n",
    "\n",
    "    def _get_random_data(self, table_name: str) -> Dict:\n",
    "        table_schema = self._get_table_schema(table_name)\n",
    "        data = dict()\n",
    "        for col in table_schema:\n",
    "            data[col] = self._get_random_value_by_type(table_schema[col])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _get_table_schema(self, table_name: str) -> Dict:\n",
    "        return self.schema[table_name][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f34a5cb-e57d-47c4-802f-dc6b0cab7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wh_read_test(wh_manager: WarehouseManager, read_operations_count: int = 1000, records_count: int = 10000,\n",
    "                 load_selective_data: bool = False, test_quantity: int = 2) -> TotalTestResult:\n",
    "    total_time = 0\n",
    "    total_memory = 0\n",
    "\n",
    "    if load_selective_data:\n",
    "        wh_manager.add_selective_data(records_count=records_count, table_name=name)\n",
    "    else:\n",
    "        wh_manager.add_random_data(records_count=records_count, table_name=name)\n",
    "\n",
    "    for _ in range(test_quantity):\n",
    "        start_time = time.time()\n",
    "        tracemalloc.start()\n",
    "        for _ in range(read_operations_count):\n",
    "            data = wh_manager.read_data(table_name=name, sort=sort_params, filter=filter_params, fetchall=False)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_memory = tracemalloc.get_traced_memory()[1]\n",
    "        tracemalloc.stop()\n",
    "        total_time += elapsed_time\n",
    "        total_memory += elapsed_memory\n",
    "    avg_time = total_time / test_quantity\n",
    "    avg_memory = total_memory / test_quantity\n",
    "    return TotalTestResult(storage_name=wh_manager.warehouse_type, used_filter=filter_params,\n",
    "                           avg_time=avg_time, avg_memory=avg_memory,\n",
    "                           load_selective_data=load_selective_data, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count)\n",
    "\n",
    "\n",
    "name = 'table_1'\n",
    "tables_schema = {'table_1': (\n",
    "    {\n",
    "        'field_1': int,\n",
    "        'field_2': str,\n",
    "        'field_3': float,\n",
    "    },\n",
    "    [\n",
    "        (('field_1', 'field_2'), (('field_3', 'desc'),)),\n",
    "        (('field_1',), (('field_3', 'desc'),)),\n",
    "        (('field_1',), ()),\n",
    "        (('field_2',), (('field_3', 'desc'),)),\n",
    "        ((), (('field_3', 'desc'),)),\n",
    "        ((), ()),\n",
    "        (('field_1', 'field_2', 'field_3'), ()),\n",
    "    ]\n",
    "), }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6accbaf6-a760-40b5-88ab-42753d977150",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_types = [WarehouseType.sqlite3, WarehouseType.duckdb, WarehouseType.polars, WarehouseType.dictdb]\n",
    "test_quantity = 2\n",
    "read_operations_count = 10000\n",
    "records_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e6fc10f-fbcb-4e8d-9f4a-94b5937d3500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_selective_data': False, 'read_operations_count': 10000, 'records_count': 1000, 'filter_params': {'field_1': 2}}\n",
      "shape: (4, 3)\n",
      "┌──────────────┬──────────┬────────────┐\n",
      "│ storage_name ┆ avg_time ┆ avg_memory │\n",
      "│ ---          ┆ ---      ┆ ---        │\n",
      "│ str          ┆ f64      ┆ f64        │\n",
      "╞══════════════╪══════════╪════════════╡\n",
      "│ dictdb       ┆ 0.044815 ┆ 1263.0     │\n",
      "│ sqlite3      ┆ 0.148322 ┆ 18586.5    │\n",
      "│ polars       ┆ 1.828073 ┆ 4045.5     │\n",
      "│ duckdb       ┆ 3.601976 ┆ 1898.5     │\n",
      "└──────────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# NOT USE SORT (not selective data)\n",
    "filter_params = {'field_1': 2}\n",
    "sort_params = {}\n",
    "load_selective_data = False\n",
    "\n",
    "statistics = []\n",
    "\n",
    "for wh_type in wh_types:\n",
    "    wh_manager = WarehouseManager(warehouse_type=wh_type, schema=tables_schema)\n",
    "    wh_manager.load_schema()\n",
    "\n",
    "    current = wh_read_test(wh_manager=wh_manager, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, load_selective_data=load_selective_data)\n",
    "\n",
    "    statistics.append(current)\n",
    "\n",
    "df = pl.DataFrame(statistics).sort('avg_time')\n",
    "print(dict(load_selective_data=load_selective_data, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, filter_params=filter_params))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df68b5a6-f8d9-4fb7-8c51-55f488dc5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_selective_data': True, 'read_operations_count': 10000, 'records_count': 1000, 'filter_params': {'field_1': 2}}\n",
      "shape: (4, 3)\n",
      "┌──────────────┬──────────┬────────────┐\n",
      "│ storage_name ┆ avg_time ┆ avg_memory │\n",
      "│ ---          ┆ ---      ┆ ---        │\n",
      "│ str          ┆ f64      ┆ f64        │\n",
      "╞══════════════╪══════════╪════════════╡\n",
      "│ dictdb       ┆ 0.046454 ┆ 436.0      │\n",
      "│ sqlite3      ┆ 0.098925 ┆ 17256.5    │\n",
      "│ polars       ┆ 1.242679 ┆ 2684.0     │\n",
      "│ duckdb       ┆ 3.402912 ┆ 626.0      │\n",
      "└──────────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# NOT USE SORT (selective data)\n",
    "filter_params = {'field_1': 2}\n",
    "sort_params = {}\n",
    "load_selective_data = True\n",
    "\n",
    "statistics = []\n",
    "\n",
    "for wh_type in wh_types:\n",
    "    wh_manager = WarehouseManager(warehouse_type=wh_type, schema=tables_schema)\n",
    "    wh_manager.load_schema()\n",
    "\n",
    "    current = wh_read_test(wh_manager=wh_manager, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, load_selective_data=load_selective_data)\n",
    "\n",
    "    statistics.append(current)\n",
    "\n",
    "df = pl.DataFrame(statistics).sort('avg_time')\n",
    "print(dict(load_selective_data=load_selective_data, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, filter_params=filter_params))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7184dc9-11e2-470f-8156-b3dc7b09a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_selective_data': False, 'read_operations_count': 10000, 'records_count': 1000, 'filter_params': {'field_1': 2}}\n",
      "shape: (4, 3)\n",
      "┌──────────────┬──────────┬────────────┐\n",
      "│ storage_name ┆ avg_time ┆ avg_memory │\n",
      "│ ---          ┆ ---      ┆ ---        │\n",
      "│ str          ┆ f64      ┆ f64        │\n",
      "╞══════════════╪══════════╪════════════╡\n",
      "│ dictdb       ┆ 0.049994 ┆ 436.0      │\n",
      "│ sqlite3      ┆ 0.170734 ┆ 17670.5    │\n",
      "│ polars       ┆ 3.244541 ┆ 3559.0     │\n",
      "│ duckdb       ┆ 5.392783 ┆ 1425.5     │\n",
      "└──────────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# USE SORT (not selective data)\n",
    "filter_params = {'field_1': 2}\n",
    "sort_params = {'field_3': 'desc'}\n",
    "load_selective_data = False\n",
    "\n",
    "statistics = []\n",
    "\n",
    "for wh_type in wh_types:\n",
    "    wh_manager = WarehouseManager(warehouse_type=wh_type, schema=tables_schema)\n",
    "    wh_manager.load_schema()\n",
    "\n",
    "    current = wh_read_test(wh_manager=wh_manager, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, load_selective_data=load_selective_data)\n",
    "\n",
    "    statistics.append(current)\n",
    "\n",
    "df = pl.DataFrame(statistics).sort('avg_time')\n",
    "print(dict(load_selective_data=load_selective_data, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, filter_params=filter_params))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "144845ae-30ca-41e0-9294-0e6191f3064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_selective_data': True, 'read_operations_count': 10000, 'records_count': 1000, 'filter_params': {'field_1': 2}}\n",
      "shape: (4, 3)\n",
      "┌──────────────┬──────────┬────────────┐\n",
      "│ storage_name ┆ avg_time ┆ avg_memory │\n",
      "│ ---          ┆ ---      ┆ ---        │\n",
      "│ str          ┆ f64      ┆ f64        │\n",
      "╞══════════════╪══════════╪════════════╡\n",
      "│ dictdb       ┆ 0.049949 ┆ 436.0      │\n",
      "│ sqlite3      ┆ 0.117103 ┆ 17488.5    │\n",
      "│ polars       ┆ 1.674742 ┆ 2902.0     │\n",
      "│ duckdb       ┆ 4.363182 ┆ 679.0      │\n",
      "└──────────────┴──────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "# USE SORT (selective data)\n",
    "filter_params = {'field_1': 2}\n",
    "sort_params = {'field_3': 'desc'}\n",
    "load_selective_data = True\n",
    "\n",
    "statistics = []\n",
    "\n",
    "for wh_type in wh_types:\n",
    "    wh_manager = WarehouseManager(warehouse_type=wh_type, schema=tables_schema)\n",
    "    wh_manager.load_schema()\n",
    "\n",
    "    current = wh_read_test(wh_manager=wh_manager, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, load_selective_data=load_selective_data)\n",
    "\n",
    "    statistics.append(current)\n",
    "\n",
    "df = pl.DataFrame(statistics).sort('avg_time')\n",
    "print(dict(load_selective_data=load_selective_data, read_operations_count=read_operations_count,\n",
    "                           records_count=records_count, filter_params=filter_params))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556df17-0c16-4b21-b9e6-c2309cf3a973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
